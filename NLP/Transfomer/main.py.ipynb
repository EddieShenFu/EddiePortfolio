{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "np.set_printoptions(suppress=True)\n",
    "logging.basicConfig(level=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NamedSplit('train'): ['newscommentary_v14',\n",
      "                       'wikititles_v1',\n",
      "                       'uncorpus_v1',\n",
      "                       'casia2015',\n",
      "                       'casict2011',\n",
      "                       'casict2015',\n",
      "                       'datum2015',\n",
      "                       'datum2017',\n",
      "                       'neu2017'],\n",
      " NamedSplit('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "  version=tfds.core.Version('0.0.3', experiments={tfds.core.Experiment.S3: False}),\n",
    "  language_pair=(\"zh\", \"en\"),\n",
    "  subsets={\n",
    "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "  }\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NamedSplit('train')(tfds.percent[0:20]),\n",
       " NamedSplit('train')(tfds.percent[20:21]),\n",
       " NamedSplit('train')(tfds.percent[21:100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perc = 20\n",
    "val_prec = 1\n",
    "drop_prec = 100 - train_perc - val_prec\n",
    "\n",
    "split = tfds.Split.TRAIN.subsplit([train_perc, val_prec, drop_prec])\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=split, as_supervised=True)\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Making Do With More', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'If the Putins, Erdo\\xc4\\x9fans, and Orb\\xc3\\xa1ns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe6\\x99\\xae\\xe4\\xba\\xac\\xe3\\x80\\x81\\xe5\\x9f\\x83\\xe5\\xb0\\x94\\xe5\\xa4\\x9a\\xe5\\xae\\x89\\xe5\\x92\\x8c\\xe6\\xac\\xa7\\xe5\\xb0\\x94\\xe7\\x8f\\xad\\xe5\\xb8\\x8c\\xe6\\x9c\\x9b\\xe7\\xbb\\xa7\\xe7\\xbb\\xad\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe5\\xbc\\x80\\xe6\\x94\\xbe\\xe5\\x9b\\xbd\\xe9\\x99\\x85\\xe4\\xbd\\x93\\xe7\\xb3\\xbb\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe7\\x9a\\x84\\xe7\\xbb\\x8f\\xe6\\xb5\\x8e\\xe5\\x88\\xa9\\xe7\\x9b\\x8a\\xef\\xbc\\x8c\\xe5\\xb0\\xb1\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe7\\xae\\x80\\xe5\\x8d\\x95\\xe5\\x9c\\xb0\\xe5\\x88\\xb6\\xe5\\xae\\x9a\\xe8\\x87\\xaa\\xe5\\xb7\\xb1\\xe7\\x9a\\x84\\xe8\\xa7\\x84\\xe5\\x88\\x99\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\x8f\\xaa\\xe6\\x9c\\x89\\xe5\\x9c\\xa8\\xe5\\x8f\\x91\\xe7\\x94\\x9f\\xe6\\xb7\\xb1\\xe5\\xba\\xa6\\xe8\\x90\\xa7\\xe6\\x9d\\xa1\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe5\\x8f\\x8d\\xe5\\xb8\\xb8\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe6\\x97\\xb6\\xef\\xbc\\x8c\\xe8\\xbf\\x99\\xe4\\xb8\\x80\\xe4\\xb8\\x8a\\xe9\\x99\\x90\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\x81\\x9a\\xe5\\x87\\xba\\xe8\\xb0\\x83\\xe6\\x95\\xb4\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe4\\xbe\\xbf\\xe8\\xae\\xa9\\xe5\\x8f\\x8d\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x94\\xbf\\xe7\\xad\\x96\\xe5\\xae\\x9e\\xe6\\x96\\xbd\\xe8\\xb6\\xb3\\xe5\\xa4\\x9f\\xe7\\x9a\\x84\\xe9\\x95\\xbf\\xe5\\xba\\xa6\\xef\\xbc\\x8c\\xe4\\xbd\\xbf\\xe4\\xba\\xba\\xe4\\xbb\\xac\\xe4\\xb8\\x80\\xe8\\x87\\xb4\\xe8\\xae\\xa4\\xe4\\xb8\\xba\\xe5\\xa2\\x9e\\xe5\\x8a\\xa0\\xe7\\x9a\\x84\\xe8\\xb5\\xa4\\xe5\\xad\\x97\\xe6\\x98\\xaf\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xef\\xbc\\x8c\\xe8\\x80\\x8c\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Do With More\n",
      "多劳应多得\n",
      "----------\n",
      "If the Putins, Erdoğans, and Orbáns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.\n",
      "如果普京、埃尔多安和欧尔班希望继续享有开放国际体系提供的经济利益，就不能简单地制定自己的规则。\n",
      "----------\n",
      "This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.\n",
      "只有在发生深度萧条或其他反常事件时，这一上限才能做出调整，以便让反周期政策实施足够的长度，使人们一致认为增加的赤字是周期性的，而不是结构性的。\n",
      "----------\n",
      "Fascist and communist regimes of the past, which followed a similar instrumentalist approach to democracy, come to mind here.\n",
      "在此我们想起了过去的法西斯主义和共产主义。 它们都相似地将民主作为实现其目的的工具。\n",
      "----------\n",
      "This phase culminated with the collapse of communism in 1989, but the chance to overcome the Continent’s historical divisions now required a redefinition of the European project.\n",
      "这种状态随着1989年共产主义崩溃而达至巅峰，但是克服欧洲大陆历史性分裂的机遇现在需要重新定义欧洲计划。\n",
      "----------\n",
      "The eurozone’s collapse (and, for all practical purposes, that of the EU itself) forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "----------\n",
      "With energy and enthusiasm, Burden turned that operation into a thriving health (not health-care) agency that covers three cities and about 300,000 people on the western edge of Los Angeles.\n",
      "在能量与激情的推动下，波顿将BCHD打造成了欣欣向荣的健康（而非医疗）机构，其服务范围覆盖了洛杉矶西侧三座城市的30万人。\n",
      "----------\n",
      "The result could be a world of fragmented blocs – an outcome that would undermine not only global prosperity, but also cooperation on shared challenges.\n",
      "其结果可能是一个四分五裂的世界 — — 这一结果不但会破坏全球繁荣，也会破坏面对共同挑战的合作。\n",
      "----------\n",
      "Among the questions being asked by NGOs, the UN, and national donors is how to prevent the recurrence of past mistakes.\n",
      "现在NGO们、联合国和捐助国们问得最多的一个问题就是如何避免再犯过去的错误。\n",
      "----------\n",
      "Managing the rise of NCDs will require long-term thinking, and government leaders will have to make investments that might pay off only after they are no longer in office.\n",
      "管理NCD的增加需要长期思维，政府领导人必须进行要在他们离任多年后才能收回成本的投资。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "  en = en_t.numpy().decode(\"utf-8\")\n",
    "  zh = zh_t.numpy().decode(\"utf-8\")\n",
    "  \n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)\n",
    "  \n",
    "  # 之後用來簡單評估模型的訓練情況\n",
    "  sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8135\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'that_', 'is_']\n",
      "\n",
      "CPU times: user 1min 30s, sys: 12.3 s, total: 1min 42s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "  print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart\n",
    "  subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2700, 7911, 10, 2942, 7457, 1163, 7925]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 2700     Taiwan\n",
      " 7911      \n",
      "   10     is \n",
      " 2942     bea\n",
      " 7457     uti\n",
      " 1163     ful\n",
      " 7925     .\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "  subword = subword_encoder_en.decode([idx])\n",
    "  print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4201\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "CPU times: user 7min 46s, sys: 11.6 s, total: 7min 58s\n",
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "  print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart \n",
    "  subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多劳应多得\n",
      "[48, 557, 116, 48, 81]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[17, 965, 11, 6, 1707, 676, 8, 211, 2712, 6683, 249, 3, 85, 1447, 7925]\n",
      "[45, 206, 171, 1, 847, 197, 236, 604, 45, 90, 17, 130, 102, 36, 7, 284, 80, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8135\n",
      "英文 EOS 的 index： 8136\n",
      "中文 BOS 的 index： 4201\n",
      "中文 EOS 的 index： 4202\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Making Do With More'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8135, 4682, 19, 717, 7911, 298, 2701, 7980, 8136],\n",
      " [4201, 48, 557, 116, 48, 81, 4202])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-23-c5074703b8a5>:5 encode  *\n        en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-81637afc26ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   2302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 2304\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   2305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m       return DatasetV1Adapter(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3888\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-23-c5074703b8a5>:5 encode  *\n        en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_examples.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([8135 4682   19  717 7911  298 2701 7980 8136], shape=(9,), dtype=int64)\n",
      "tf.Tensor([4201   48  557  116   48   81 4202], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29914 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "  cond1 = len(en_indices) <= MAX_LENGTH\n",
    "  cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "  assert cond1 and cond2\n",
    "  num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8135 4682   19 ...    0    0    0]\n",
      " [8135   17  965 ... 8136    0    0]\n",
      " [8135 6602    2 ...    0    0    0]\n",
      " ...\n",
      " [8135 1097  270 ...    0    0    0]\n",
      " [8135 1713   70 ...    0    0    0]\n",
      " [8135 2731 4553 ...    0    0    0]], shape=(64, 34), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4201   48  557 ...    0    0    0]\n",
      " [4201   45  206 ...    0    0    0]\n",
      " [4201   58    5 ...  683    3 4202]\n",
      " ...\n",
      " [4201   29  120 ...    0    0    0]\n",
      " [4201  297  161 ...    0    0    0]\n",
      " [4201  279  149 ... 4202    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8135  264  559 ...    0    0    0]\n",
      " [8135   39   41 ...    0    0    0]\n",
      " [8135   45  409 ...    0    0    0]\n",
      " ...\n",
      " [8135   45  346 ...    0    0    0]\n",
      " [8135   17 1410 ...    0    0    0]\n",
      " [8135   89   10 ...    0    0    0]], shape=(128, 40), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4201  127   20 ...    0    0    0]\n",
      " [4201   35    6 ...    0    0    0]\n",
      " [4201  120  340 ...    0    0    0]\n",
      " ...\n",
      " [4201   15   14 ...    0    0    0]\n",
      " [4201   45   90 ...    0    0    0]\n",
      " [4201  240   54 ...    3 4202    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.00872516,  0.03628476, -0.03393974,  0.00709321],\n",
       "         [-0.03939278, -0.0301337 , -0.00684045,  0.03930915],\n",
       "         [ 0.04217876,  0.03556073,  0.00940926,  0.02619899],\n",
       "         [ 0.00395658, -0.02045913,  0.04225378,  0.03054985],\n",
       "         [-0.0017071 , -0.01169461, -0.03898996,  0.02977444],\n",
       "         [-0.01053456, -0.02597178,  0.0221846 , -0.03926188],\n",
       "         [ 0.00246267, -0.0197565 ,  0.03870387,  0.04282421],\n",
       "         [ 0.00246267, -0.0197565 ,  0.03870387,  0.04282421]],\n",
       " \n",
       "        [[-0.00872516,  0.03628476, -0.03393974,  0.00709321],\n",
       "         [-0.02387941,  0.03241142, -0.02551947, -0.04527463],\n",
       "         [-0.01562656,  0.01015867, -0.00413325, -0.02324653],\n",
       "         [-0.03823967, -0.01697657, -0.03591858,  0.03328835],\n",
       "         [-0.03155795,  0.00274887,  0.04581268,  0.04479903],\n",
       "         [ 0.00292424, -0.04440271, -0.01196776,  0.04736396],\n",
       "         [-0.0017071 , -0.01169461, -0.03898996,  0.02977444],\n",
       "         [-0.01053456, -0.02597178,  0.0221846 , -0.03926188]]],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[-0.03401262, -0.03672125,  0.00148769, -0.02200452],\n",
       "         [-0.00802097, -0.04834964, -0.02985298, -0.02165037],\n",
       "         [-0.04805389,  0.03293635, -0.03475545, -0.01825899],\n",
       "         [-0.01045249,  0.03329812,  0.01279325, -0.03139227],\n",
       "         [-0.00984706, -0.02574105, -0.01065782,  0.01140115],\n",
       "         [ 0.04679844,  0.01800437,  0.00799765,  0.04974586],\n",
       "         [ 0.00132418, -0.03924266, -0.00341817, -0.03768504],\n",
       "         [-0.00556985, -0.04969132, -0.04252383, -0.02408794],\n",
       "         [-0.00556985, -0.04969132, -0.04252383, -0.02408794],\n",
       "         [-0.00556985, -0.04969132, -0.04252383, -0.02408794]],\n",
       " \n",
       "        [[-0.03401262, -0.03672125,  0.00148769, -0.02200452],\n",
       "         [ 0.00608138,  0.03791398, -0.00718598,  0.00104474],\n",
       "         [-0.01411204,  0.02962438,  0.01688961,  0.02466543],\n",
       "         [ 0.03541957,  0.00966803,  0.0428454 ,  0.03259467],\n",
       "         [ 0.02886103,  0.03402147, -0.00361333, -0.03875045],\n",
       "         [ 0.00446288, -0.02858969,  0.0252871 ,  0.01449865],\n",
       "         [-0.03010942, -0.03628191, -0.03144275, -0.04487304],\n",
       "         [ 0.00266431, -0.03812676,  0.02856315,  0.04966343],\n",
       "         [ 0.04679844,  0.01800437,  0.00799765,  0.04974586],\n",
       "         [ 0.00132418, -0.03924266, -0.00341817, -0.03768504]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      " [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      " [-0.00556985 -0.04969132 -0.04252383 -0.02408794]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # 將 `q`、 `k` 做點積再 scale\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "  # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # 以注意權重對 v 做加權平均（weighted average）\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.37518066 0.37483087 0.3749091  0.4999044 ]\n",
      "  [0.37498498 0.3751402  0.3751197  0.4999625 ]\n",
      "  [0.37501425 0.37473822 0.37502044 0.49995366]\n",
      "  [0.37481582 0.37508464 0.3751502  0.50004935]\n",
      "  [0.37512243 0.37493312 0.37505168 0.49990326]\n",
      "  [0.37488103 0.3752157  0.37497765 0.5001095 ]\n",
      "  [0.37483    0.37506163 0.37517226 0.500027  ]\n",
      "  [0.37483    0.37506163 0.37517226 0.500027  ]]\n",
      "\n",
      " [[0.62518704 0.24983297 0.6250395  0.3749666 ]\n",
      "  [0.6252012  0.24961075 0.62512374 0.37471482]\n",
      "  [0.6250589  0.24984503 0.6250475  0.3748681 ]\n",
      "  [0.62508297 0.250073   0.6250168  0.37506855]\n",
      "  [0.62479925 0.25036332 0.6248135  0.37528878]\n",
      "  [0.62492156 0.25029588 0.6249556  0.37520885]\n",
      "  [0.625098   0.2500403  0.62503004 0.3750695 ]\n",
      "  [0.6249098  0.24994117 0.62504107 0.37484056]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.12518652 0.12500928 0.12507352 0.1248996  0.12509455 0.12490661\n",
      "   0.12491497 0.12491497]\n",
      "  [0.12491778 0.12518606 0.12482233 0.12501846 0.12504874 0.12490162\n",
      "   0.1250525  0.1250525 ]\n",
      "  [0.1250259  0.12486619 0.12521544 0.12501644 0.12497193 0.12483996\n",
      "   0.12503207 0.12503207]\n",
      "  [0.12480042 0.12501068 0.12496474 0.12512203 0.12489336 0.12493914\n",
      "   0.12513483 0.12513483]\n",
      "  [0.12504514 0.1250909  0.12497015 0.12494324 0.12513404 0.12486787\n",
      "   0.12497437 0.12497437]\n",
      "  [0.12491287 0.12499937 0.12489377 0.1250447  0.12492347 0.12520683\n",
      "   0.12500949 0.12500949]\n",
      "  [0.12480078 0.12502968 0.12496535 0.12511979 0.12490946 0.12488896\n",
      "   0.12514298 0.12514298]\n",
      "  [0.12480078 0.12502968 0.12496535 0.12511979 0.12490946 0.12488896\n",
      "   0.12514298 0.12514298]]\n",
      "\n",
      " [[0.1251336  0.12509197 0.12500137 0.12504466 0.1249175  0.12491547\n",
      "   0.12504166 0.12485382]\n",
      "  [0.12510405 0.1252538  0.12509975 0.12496921 0.12483636 0.12477439\n",
      "   0.12494024 0.12502226]\n",
      "  [0.12502307 0.12510937 0.12504962 0.12498054 0.1249487  0.12489633\n",
      "   0.12495412 0.12503818]\n",
      "  [0.12499558 0.12490809 0.12490979 0.1251817  0.12498514 0.12508784\n",
      "   0.12508827 0.12484362]\n",
      "  [0.12492548 0.12483229 0.12493499 0.12504221 0.12529902 0.1250643\n",
      "   0.12495244 0.12494925]\n",
      "  [0.124913   0.12475985 0.12487216 0.12513448 0.12505384 0.12524202\n",
      "   0.12511837 0.12490624]\n",
      "  [0.12502918 0.12491568 0.12491994 0.12512489 0.12493196 0.12510835\n",
      "   0.12511808 0.12485193]\n",
      "  [0.12489939 0.1250558  0.1250621  0.12493823 0.12498687 0.12495431\n",
      "   0.12490998 0.12519331]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16687751 0.16664125 0.16672687 0.16649503 0.1667549  0.16650438\n",
      "   0.         0.        ]\n",
      "  [0.16658036 0.16693811 0.16645308 0.16671462 0.16675499 0.16655882\n",
      "   0.         0.        ]\n",
      "  [0.16671546 0.16650249 0.1669682  0.16670282 0.16664349 0.16646752\n",
      "   0.         0.        ]\n",
      "  [0.16646041 0.16674086 0.16667958 0.16688935 0.16658437 0.16664544\n",
      "   0.         0.        ]\n",
      "  [0.16671544 0.16677645 0.16661547 0.16657959 0.16683397 0.1664791\n",
      "   0.         0.        ]\n",
      "  [0.16655472 0.16667004 0.16652924 0.16673048 0.16656885 0.16694666\n",
      "   0.         0.        ]\n",
      "  [0.16646451 0.16676985 0.16668403 0.16689003 0.16660948 0.16658214\n",
      "   0.         0.        ]\n",
      "  [0.16646451 0.16676985 0.16668403 0.16689003 0.16660948 0.16658214\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.1251336  0.12509197 0.12500137 0.12504466 0.1249175  0.12491547\n",
      "   0.12504166 0.12485382]\n",
      "  [0.12510405 0.1252538  0.12509975 0.12496921 0.12483636 0.12477439\n",
      "   0.12494024 0.12502226]\n",
      "  [0.12502307 0.12510937 0.12504962 0.12498054 0.1249487  0.12489633\n",
      "   0.12495412 0.12503818]\n",
      "  [0.12499558 0.12490809 0.12490979 0.1251817  0.12498514 0.12508784\n",
      "   0.12508827 0.12484362]\n",
      "  [0.12492548 0.12483229 0.12493499 0.12504221 0.12529902 0.1250643\n",
      "   0.12495244 0.12494925]\n",
      "  [0.124913   0.12475985 0.12487216 0.12513448 0.12505384 0.12524202\n",
      "   0.12511837 0.12490624]\n",
      "  [0.12502918 0.12491568 0.12491994 0.12512489 0.12493196 0.12510835\n",
      "   0.12511808 0.12485193]\n",
      "  [0.12489939 0.1250558  0.1250621  0.12493823 0.12498687 0.12495431\n",
      "   0.12490998 0.12519331]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12504166, 0.12485382],\n",
       "        [0.12494024, 0.12502226],\n",
       "        [0.12495412, 0.12503818],\n",
       "        [0.12508827, 0.12484362],\n",
       "        [0.12495244, 0.12494925],\n",
       "        [0.12511837, 0.12490624],\n",
       "        [0.12511808, 0.12485193],\n",
       "        [0.12490998, 0.12519331]]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [-0.00802097 -0.04834964 -0.02985298 -0.02165037]\n",
      "  [-0.04805389  0.03293635 -0.03475545 -0.01825899]\n",
      "  [-0.01045249  0.03329812  0.01279325 -0.03139227]\n",
      "  [-0.00984706 -0.02574105 -0.01065782  0.01140115]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]]\n",
      "\n",
      " [[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [ 0.00608138  0.03791398 -0.00718598  0.00104474]\n",
      "  [-0.01411204  0.02962438  0.01688961  0.02466543]\n",
      "  [ 0.03541957  0.00966803  0.0428454   0.03259467]\n",
      "  [ 0.02886103  0.03402147 -0.00361333 -0.03875045]\n",
      "  [ 0.00446288 -0.02858969  0.0252871   0.01449865]\n",
      "  [-0.03010942 -0.03628191 -0.03144275 -0.04487304]\n",
      "  [ 0.00266431 -0.03812676  0.02856315  0.04966343]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.4998398  0.5001602  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33313262 0.33304116 0.3338263  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24989563 0.24976186 0.25013125 0.25021127 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.20006233 0.2001005  0.19993976 0.19983621 0.2000613  0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16646475 0.16652916 0.16650583 0.16663018 0.16670573 0.1671644\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14296137 0.14300315 0.14276354 0.14278981 0.14284588 0.14262104\n",
      "   0.14301524 0.         0.         0.        ]\n",
      "  [0.12505664 0.12516657 0.12493589 0.1248152  0.1249961  0.1247335\n",
      "   0.12508884 0.12520729 0.         0.        ]\n",
      "  [0.11114098 0.11123868 0.11103366 0.1109264  0.11108718 0.1108538\n",
      "   0.1111696  0.11127487 0.11127487 0.        ]\n",
      "  [0.10001215 0.10010006 0.09991557 0.09981905 0.09996373 0.09975372\n",
      "   0.1000379  0.10013262 0.10013262 0.10013262]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.499605   0.500395   0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33304656 0.33339098 0.3335625  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24960926 0.24992414 0.2500499  0.25041673 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19979988 0.20008326 0.19989654 0.19993149 0.20028886 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16670558 0.16655228 0.16664375 0.16677403 0.16652949 0.16679482\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14309436 0.14276016 0.14269535 0.14255705 0.14284074 0.1428199\n",
      "   0.14323239 0.         0.         0.        ]\n",
      "  [0.12498485 0.1248698  0.12500243 0.12512924 0.12476588 0.12512773\n",
      "   0.12485477 0.12526537 0.         0.        ]\n",
      "  [0.11088934 0.11112785 0.11114305 0.11128544 0.1110748  0.11110874\n",
      "   0.11082225 0.11119319 0.11135541 0.        ]\n",
      "  [0.10010771 0.09992195 0.09988828 0.09991132 0.10000543 0.1000214\n",
      "   0.10015589 0.09997317 0.0998694  0.10014543]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.4998398, 0.5001602, 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       ],\n",
       "       [0.499605 , 0.500395 , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.03939278 -0.0301337  -0.00684045  0.03930915]\n",
      "  [ 0.04217876  0.03556073  0.00940926  0.02619899]\n",
      "  [ 0.00395658 -0.02045913  0.04225378  0.03054985]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]]\n",
      "\n",
      " [[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.02387941  0.03241142 -0.02551947 -0.04527463]\n",
      "  [-0.01562656  0.01015867 -0.00413325 -0.02324653]\n",
      "  [-0.03823967 -0.01697657 -0.03591858  0.03328835]\n",
      "  [-0.03155795  0.00274887  0.04581268  0.04479903]\n",
      "  [ 0.00292424 -0.04440271 -0.01196776  0.04736396]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.00872516  0.03628476]\n",
      "   [-0.03939278 -0.0301337 ]\n",
      "   [ 0.04217876  0.03556073]\n",
      "   [ 0.00395658 -0.02045913]\n",
      "   [-0.0017071  -0.01169461]\n",
      "   [-0.01053456 -0.02597178]\n",
      "   [ 0.00246267 -0.0197565 ]\n",
      "   [ 0.00246267 -0.0197565 ]]\n",
      "\n",
      "  [[-0.03393974  0.00709321]\n",
      "   [-0.00684045  0.03930915]\n",
      "   [ 0.00940926  0.02619899]\n",
      "   [ 0.04225378  0.03054985]\n",
      "   [-0.03898996  0.02977444]\n",
      "   [ 0.0221846  -0.03926188]\n",
      "   [ 0.03870387  0.04282421]\n",
      "   [ 0.03870387  0.04282421]]]\n",
      "\n",
      "\n",
      " [[[-0.00872516  0.03628476]\n",
      "   [-0.02387941  0.03241142]\n",
      "   [-0.01562656  0.01015867]\n",
      "   [-0.03823967 -0.01697657]\n",
      "   [-0.03155795  0.00274887]\n",
      "   [ 0.00292424 -0.04440271]\n",
      "   [-0.0017071  -0.01169461]\n",
      "   [-0.01053456 -0.02597178]]\n",
      "\n",
      "  [[-0.03393974  0.00709321]\n",
      "   [-0.02551947 -0.04527463]\n",
      "   [-0.00413325 -0.02324653]\n",
      "   [-0.03591858  0.03328835]\n",
      "   [ 0.04581268  0.04479903]\n",
      "   [-0.01196776  0.04736396]\n",
      "   [-0.03898996  0.02977444]\n",
      "   [ 0.0221846  -0.03926188]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "  # x.shape: (batch_size, seq_len, d_model)\n",
    "  batch_size = tf.shape(x)[0]\n",
    "  \n",
    "  # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "  assert d_model % num_heads == 0\n",
    "  depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "  # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "  # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "  # (batch_size, seq_len, num_heads, depth)\n",
    "  reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "  # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "  # (batch_size, num_heads, seq_len, depth)\n",
    "  output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "  \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[-0.01000656  0.0136093   0.01146611 -0.0000503 ]\n",
      "  [-0.00998217  0.01357744  0.01145472 -0.00002868]\n",
      "  [-0.00996785  0.01356275  0.0114211  -0.00005009]\n",
      "  [-0.00995941  0.01354941  0.01142498 -0.00002872]\n",
      "  [-0.00998485  0.01357907  0.01144329 -0.00004433]\n",
      "  [-0.00999995  0.01359164  0.01145778 -0.00003175]\n",
      "  [-0.00995556  0.01354592  0.01142278 -0.00002877]\n",
      "  [-0.00995556  0.01354592  0.01142278 -0.00002877]]\n",
      "\n",
      " [[ 0.00610415 -0.01002899 -0.01386858  0.00167259]\n",
      "  [ 0.00606181 -0.00998417 -0.01382717  0.00168069]\n",
      "  [ 0.00609498 -0.01002481 -0.01385703  0.00168881]\n",
      "  [ 0.00613258 -0.01006723 -0.01388553  0.00169558]\n",
      "  [ 0.00615591 -0.01009651 -0.01390056  0.00171272]\n",
      "  [ 0.00617421 -0.01011746 -0.01393264  0.00169231]\n",
      "  [ 0.00614059 -0.01007446 -0.01390366  0.00167899]\n",
      "  [ 0.00610405 -0.01004137 -0.01386531  0.0017017 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.03939278 -0.0301337  -0.00684045  0.03930915]\n",
      "  [ 0.04217876  0.03556073  0.00940926  0.02619899]\n",
      "  [ 0.00395658 -0.02045913  0.04225378  0.03054985]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]]\n",
      "\n",
      " [[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.02387941  0.03241142 -0.02551947 -0.04527463]\n",
      "  [-0.01562656  0.01015867 -0.00413325 -0.02324653]\n",
      "  [-0.03823967 -0.01697657 -0.03591858  0.03328835]\n",
      "  [-0.03155795  0.00274887  0.04581268  0.04479903]\n",
      "  [ 0.00292424 -0.04440271 -0.01196776  0.04736396]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.81658655  1.4364216  -1.044008    0.424173  ]\n",
      "  [-1.1373439  -0.13116369 -0.33648854  1.6049961 ]\n",
      "  [-0.07380982  1.1580226  -1.5584166   0.47420377]\n",
      "  [-0.22442523 -1.4397199   0.33601767  1.3281274 ]\n",
      "  [-0.2970155   0.57074887 -1.468887    1.1951536 ]\n",
      "  [ 0.4581663  -0.404145    1.3193328  -1.3733542 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]]\n",
      "\n",
      " [[-0.8516326   1.4478076  -1.0054984   0.4093235 ]\n",
      "  [-0.5616184   1.5969627  -1.0646287   0.02928439]\n",
      "  [-0.96634096  1.6015658  -0.7045317   0.06930672]\n",
      "  [-1.12946     0.7941866  -0.84166455  1.176938  ]\n",
      "  [-1.645265    0.25666368  0.33159477  1.0570066 ]\n",
      "  [ 0.10427634 -0.94059813 -0.7554399   1.5917617 ]\n",
      "  [-0.4170378   0.67471915 -1.4206665   1.1629851 ]\n",
      "  [ 0.26112485 -0.22908711  1.3766065  -1.4086442 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    # 3 個 sub-layers 的主角們\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    # 定義每個 sub-layer 用的 LayerNorm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 定義每個 sub-layer 用的 Dropout\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "    # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "    # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "    # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "    # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "    # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "    # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "    # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [-0.00802097 -0.04834964 -0.02985298 -0.02165037]\n",
      "  [-0.04805389  0.03293635 -0.03475545 -0.01825899]\n",
      "  [-0.01045249  0.03329812  0.01279325 -0.03139227]\n",
      "  [-0.00984706 -0.02574105 -0.01065782  0.01140115]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]]\n",
      "\n",
      " [[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [ 0.00608138  0.03791398 -0.00718598  0.00104474]\n",
      "  [-0.01411204  0.02962438  0.01688961  0.02466543]\n",
      "  [ 0.03541957  0.00966803  0.0428454   0.03259467]\n",
      "  [ 0.02886103  0.03402147 -0.00361333 -0.03875045]\n",
      "  [ 0.00446288 -0.02858969  0.0252871   0.01449865]\n",
      "  [-0.03010942 -0.03628191 -0.03144275 -0.04487304]\n",
      "  [ 0.00266431 -0.03812676  0.02856315  0.04966343]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.81658655  1.4364216  -1.044008    0.424173  ]\n",
      "  [-1.1373439  -0.13116369 -0.33648854  1.6049961 ]\n",
      "  [-0.07380982  1.1580226  -1.5584166   0.47420377]\n",
      "  [-0.22442523 -1.4397199   0.33601767  1.3281274 ]\n",
      "  [-0.2970155   0.57074887 -1.468887    1.1951536 ]\n",
      "  [ 0.4581663  -0.404145    1.3193328  -1.3733542 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]]\n",
      "\n",
      " [[-0.8516326   1.4478076  -1.0054984   0.4093235 ]\n",
      "  [-0.5616184   1.5969627  -1.0646287   0.02928439]\n",
      "  [-0.96634096  1.6015658  -0.7045317   0.06930672]\n",
      "  [-1.12946     0.7941866  -0.84166455  1.176938  ]\n",
      "  [-1.645265    0.25666368  0.33159477  1.0570066 ]\n",
      "  [ 0.10427634 -0.94059813 -0.7554399   1.5917617 ]\n",
      "  [-0.4170378   0.67471915 -1.4206665   1.1629851 ]\n",
      "  [ 0.26112485 -0.22908711  1.3766065  -1.4086442 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[ 0.1504772  -1.6583127   0.54111654  0.966719  ]\n",
      "  [ 0.89563245 -1.594563   -0.10383192  0.80276245]\n",
      "  [-0.97793126  1.4678713   0.37510243 -0.86504257]\n",
      "  [-0.31977004  0.8672132   0.9501734  -1.4976165 ]\n",
      "  [ 0.4259625  -1.3991169  -0.34634256  1.3194968 ]\n",
      "  [ 0.8982514  -0.82325643 -1.1583406   1.0833455 ]\n",
      "  [ 1.2488142  -1.5478094   0.1445741   0.15442108]\n",
      "  [ 1.1827304  -1.2235019  -0.72236496  0.76313657]\n",
      "  [ 1.1827304  -1.2235022  -0.7223651   0.7631367 ]\n",
      "  [ 1.1827304  -1.2235022  -0.7223651   0.7631367 ]]\n",
      "\n",
      " [[ 0.1413997  -1.6384233   0.45268044  1.0443432 ]\n",
      "  [-0.15006247  1.616906   -1.1124184  -0.35442516]\n",
      "  [-1.6674784   0.9412033   0.17617717  0.550098  ]\n",
      "  [ 0.9220374  -1.5413343  -0.22996673  0.84926355]\n",
      "  [ 1.3146342   0.62156224 -0.89404845 -1.042148  ]\n",
      "  [ 0.41705227 -1.6228238   0.12041     1.0853614 ]\n",
      "  [ 1.3460523  -1.4763024   0.05538279  0.07486729]\n",
      "  [ 0.26538524 -1.4889268  -0.08254118  1.3060827 ]\n",
      "  [ 0.7809489  -0.82808274 -1.1390346   1.1861686 ]\n",
      "  [ 1.2466564  -1.5441262   0.0557915   0.24167821]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "    # 建立 `num_layers` 個 EncoderLayers\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "    # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "    # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "    input_seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "    # 再加上對應長度的位置編碼\n",
    "    x = self.embedding(x)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "    # 對 embedding 跟位置編碼的總合做 regularization\n",
    "    # 這在 Decoder 也會做\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    # 通過 N 個 EncoderLayer 做編碼\n",
    "    for i, enc_layer in enumerate(self.enc_layers):\n",
    "      x = enc_layer(x, training, mask)\n",
    "      # 以下只是用來 demo EncoderLayer outputs\n",
    "      #print('-' * 20)\n",
    "      #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "      \n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.80654085 -0.5846039  -0.3143984   1.7055432 ]\n",
      "  [-0.46891177 -0.5740812  -0.68403816  1.7270309 ]\n",
      "  [-0.3197091  -0.17782497 -1.1191479   1.6166819 ]\n",
      "  [-0.49274147  0.2699072  -1.2412686   1.464103  ]\n",
      "  [-0.88477206  0.16279443 -0.8493917   1.5713693 ]\n",
      "  [-0.9662535  -0.25279236 -0.45335218  1.6723982 ]\n",
      "  [-0.84764326 -0.5615214  -0.2887244   1.6978891 ]\n",
      "  [-0.61957765 -0.59192634 -0.51938546  1.7308894 ]]\n",
      "\n",
      " [[-0.80838835 -0.56457365 -0.33460823  1.7075703 ]\n",
      "  [-0.50152004 -0.5214133  -0.7037289   1.7266624 ]\n",
      "  [-0.34244877 -0.11313806 -1.144456    1.6000429 ]\n",
      "  [-0.50724393  0.21401617 -1.205033    1.4982607 ]\n",
      "  [-0.8861126   0.26368496 -0.9036027   1.5260304 ]\n",
      "  [-0.96629447 -0.21083693 -0.49055362  1.667685  ]\n",
      "  [-0.8683281  -0.53832126 -0.28836071  1.6950102 ]\n",
      "  [-0.6246324  -0.5758679  -0.5305908   1.7310913 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    # 為中文（目標語言）建立詞嵌入層\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  \n",
    "  # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    \n",
    "    tar_seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "    \n",
    "    # 這邊跟 Encoder 做的事情完全一樣\n",
    "    x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    \n",
    "    for i, dec_layer in enumerate(self.dec_layers):\n",
    "      x, block1, block2 = dec_layer(x, enc_output, training,\n",
    "                                    combined_mask, inp_padding_mask)\n",
    "      \n",
    "      # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.80654085 -0.5846039  -0.3143984   1.7055432 ]\n",
      "  [-0.46891177 -0.5740812  -0.68403816  1.7270309 ]\n",
      "  [-0.3197091  -0.17782497 -1.1191479   1.6166819 ]\n",
      "  [-0.49274147  0.2699072  -1.2412686   1.464103  ]\n",
      "  [-0.88477206  0.16279443 -0.8493917   1.5713693 ]\n",
      "  [-0.9662535  -0.25279236 -0.45335218  1.6723982 ]\n",
      "  [-0.84764326 -0.5615214  -0.2887244   1.6978891 ]\n",
      "  [-0.61957765 -0.59192634 -0.51938546  1.7308894 ]]\n",
      "\n",
      " [[-0.80838835 -0.56457365 -0.33460823  1.7075703 ]\n",
      "  [-0.50152004 -0.5214133  -0.7037289   1.7266624 ]\n",
      "  [-0.34244877 -0.11313806 -1.144456    1.6000429 ]\n",
      "  [-0.50724393  0.21401617 -1.205033    1.4982607 ]\n",
      "  [-0.8861126   0.26368496 -0.9036027   1.5260304 ]\n",
      "  [-0.96629447 -0.21083693 -0.49055362  1.667685  ]\n",
      "  [-0.8683281  -0.53832126 -0.28836071  1.6950102 ]\n",
      "  [-0.6246324  -0.5758679  -0.5305908   1.7310913 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "inp_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[-0.54376316 -1.0559629   1.609091   -0.00936503]\n",
      "  [-0.35729462 -1.2363737   1.5295788   0.06408951]\n",
      "  [ 0.3595041  -1.4217519   1.3327445  -0.27049667]\n",
      "  [ 0.00910443 -1.3681055   1.4556322  -0.09663111]\n",
      "  [-0.3984219  -1.0891634   1.6237148  -0.13612941]\n",
      "  [-0.41910928 -1.0254463   1.6521795  -0.20762378]\n",
      "  [-0.3679743  -1.0361041   1.6521348  -0.24805656]\n",
      "  [-0.19375195 -1.1218891   1.6165613  -0.3009202 ]\n",
      "  [ 0.40127635 -1.3597699   1.3540745  -0.3955807 ]\n",
      "  [ 0.17590103 -1.4190679   1.3905344  -0.14736763]]\n",
      "\n",
      " [[-0.54991776 -1.0509206   1.6102996  -0.00946119]\n",
      "  [-0.37900764 -1.2450974   1.514628    0.10947721]\n",
      "  [ 0.17467734 -1.3877552   1.415193   -0.20211513]\n",
      "  [-0.03870562 -1.337597    1.4825784  -0.10627589]\n",
      "  [-0.4350825  -1.0675747   1.6293935  -0.12673639]\n",
      "  [-0.41048306 -1.0317236   1.6503687  -0.2081619 ]\n",
      "  [-0.3626595  -1.0360833   1.6524631  -0.25372016]\n",
      "  [-0.24817821 -1.1092765   1.623865   -0.26641032]\n",
      "  [ 0.18505694 -1.3670969   1.4271387  -0.24509867]\n",
      "  [ 0.09142642 -1.3988855   1.4218551  -0.11439608]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "decoder_layer1_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer1_block2.shape: (2, 2, 10, 8)\n",
      "decoder_layer2_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer2_block2.shape: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Decoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_padding_mask:\", inp_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "dec_out, attn = decoder(tar, enc_out, training=False, \n",
    "                        combined_mask=combined_mask,\n",
    "                        inp_padding_mask=inp_padding_mask)\n",
    "print(\"dec_out:\", dec_out)\n",
    "print(\"-\" * 20)\n",
    "for block_name, attn_weights in attn.items():\n",
    "  print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "class Transformer(tf.keras.Model):\n",
    "  # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, rate)\n",
    "    # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "  # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "  # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           combined_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "    \n",
    "    # 將 Decoder 輸出通過最後一個 linear layer\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_inp: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "tar_real: tf.Tensor(\n",
      "[[  10  241   80   27    3 4202    0    0    0]\n",
      " [ 162  467  421  189   14    7  553    3 4202]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "predictions: tf.Tensor(\n",
      "[[[ 0.00929453 -0.01123782  0.05421777 ... -0.01170466  0.00628542\n",
      "   -0.07576237]\n",
      "  [ 0.03640017 -0.0188504   0.0511385  ... -0.02349907  0.01716621\n",
      "   -0.06729947]\n",
      "  [ 0.05617092 -0.02265773  0.04667147 ... -0.02913138  0.02415059\n",
      "   -0.05331098]\n",
      "  ...\n",
      "  [ 0.00905135 -0.01058669  0.05486143 ... -0.01039154  0.0058039\n",
      "   -0.07445521]\n",
      "  [ 0.02215609 -0.01478041  0.05375389 ... -0.0170105   0.01135763\n",
      "   -0.07241639]\n",
      "  [ 0.04786561 -0.02148081  0.04837158 ... -0.02759764  0.02148173\n",
      "   -0.06043392]]\n",
      "\n",
      " [[ 0.00996658 -0.01115559  0.05453675 ... -0.0114185   0.00637142\n",
      "   -0.07500791]\n",
      "  [ 0.03897631 -0.01930442  0.0508956  ... -0.02409907  0.01803425\n",
      "   -0.06564321]\n",
      "  [ 0.05387273 -0.02244362  0.04702405 ... -0.02893806  0.02348556\n",
      "   -0.05554678]\n",
      "  ...\n",
      "  [ 0.01048943 -0.01085558  0.05502524 ... -0.01070841  0.0062833\n",
      "   -0.0738526 ]\n",
      "  [ 0.02370837 -0.01504853  0.05381611 ... -0.01732858  0.01186723\n",
      "   -0.07158873]\n",
      "  [ 0.04920106 -0.02166032  0.0481827  ... -0.02781233  0.02190086\n",
      "   -0.05933255]]], shape=(2, 9, 4203), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, \n",
    "                          input_vocab_size, output_vocab_size)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n",
    "                                        combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_inp:\", tar_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_real:\", tar_real)\n",
    "print(\"-\" * 20)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.31326166, 0.31326166, 1.3132616 ], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label\n",
    "real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)\n",
    "pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)\n",
    "loss_object(real, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tf.Tensor(\n",
      "[[[ 0.00929453 -0.01123782  0.05421777 ... -0.01170466  0.00628542\n",
      "   -0.07576237]\n",
      "  [ 0.03640017 -0.0188504   0.0511385  ... -0.02349907  0.01716621\n",
      "   -0.06729947]\n",
      "  [ 0.05617092 -0.02265773  0.04667147 ... -0.02913138  0.02415059\n",
      "   -0.05331098]\n",
      "  ...\n",
      "  [ 0.00905135 -0.01058669  0.05486143 ... -0.01039154  0.0058039\n",
      "   -0.07445521]\n",
      "  [ 0.02215609 -0.01478041  0.05375389 ... -0.0170105   0.01135763\n",
      "   -0.07241639]\n",
      "  [ 0.04786561 -0.02148081  0.04837158 ... -0.02759764  0.02148173\n",
      "   -0.06043392]]\n",
      "\n",
      " [[ 0.00996658 -0.01115559  0.05453675 ... -0.0114185   0.00637142\n",
      "   -0.07500791]\n",
      "  [ 0.03897631 -0.01930442  0.0508956  ... -0.02409907  0.01803425\n",
      "   -0.06564321]\n",
      "  [ 0.05387273 -0.02244362  0.04702405 ... -0.02893806  0.02348556\n",
      "   -0.05554678]\n",
      "  ...\n",
      "  [ 0.01048943 -0.01085558  0.05502524 ... -0.01070841  0.0062833\n",
      "   -0.0738526 ]\n",
      "  [ 0.02370837 -0.01504853  0.05381611 ... -0.01732858  0.01186723\n",
      "   -0.07158873]\n",
      "  [ 0.04920106 -0.02166032  0.0481827  ... -0.02781233  0.02190086\n",
      "   -0.05933255]]], shape=(2, 9, 4203), dtype=float32)\n",
      "--------------------\n",
      "tf.Tensor(\n",
      "[[1.4971999 3.1899033 4.1454935 3.7353938 2.8697395 1.8605262 1.3746359\n",
      "  2.277917  3.8190799]\n",
      " [1.4881061 3.3035862 4.0757217 3.7524652 2.8363166 1.9132916 1.4376439\n",
      "  2.3432949 3.8689973]], shape=(2, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions:\", predictions)\n",
    "print(\"-\" * 20)\n",
    "print(tf.reduce_sum(predictions, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # 照樣計算所有位置的 cross entropy 但不加總\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 8137\n",
      "target_vocab_size: 4203\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # 論文預設 `warmup_steps` = 4000\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "  schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "  labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "  plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "           label=label, color=colors[i // 3])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這個 Transformer 有 4 層 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 8137\n",
      "target_vocab_size: 4203\n",
      "dropout_rate: 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒找到 checkpoint，從頭訓練。\n"
     ]
    }
   ],
   "source": [
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "  # 用來確認之前訓練多少 epochs 了\n",
    "  last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "  print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "  last_epoch = 0\n",
    "  print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "def create_masks(inp, tar):\n",
    "  # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "  # 關注 Encoder 輸出序列用的\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "  # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "  # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  # 建立 3 個遮罩\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 0 epochs。\n",
      "剩餘 epochs：-30\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function train_step at 0x147d929e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function train_step at 0x147d929e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 1 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-1\n",
      "Epoch 1 Loss 5.1926 Accuracy 0.0195\n",
      "Time taken for 1 epoch: 288.58325695991516 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-2\n",
      "Epoch 2 Loss 4.2485 Accuracy 0.0599\n",
      "Time taken for 1 epoch: 250.06949520111084 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-3\n",
      "Epoch 3 Loss 3.7378 Accuracy 0.0998\n",
      "Time taken for 1 epoch: 230.621248960495 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-4\n",
      "Epoch 4 Loss 3.2605 Accuracy 0.1534\n",
      "Time taken for 1 epoch: 236.94213795661926 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-5\n",
      "Epoch 5 Loss 2.9613 Accuracy 0.1826\n",
      "Time taken for 1 epoch: 233.04599022865295 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-6\n",
      "Epoch 6 Loss 2.7736 Accuracy 0.1991\n",
      "Time taken for 1 epoch: 217.76713585853577 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-7\n",
      "Epoch 7 Loss 2.6319 Accuracy 0.2123\n",
      "Time taken for 1 epoch: 234.3578860759735 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-8\n",
      "Epoch 8 Loss 2.5177 Accuracy 0.2239\n",
      "Time taken for 1 epoch: 252.82849097251892 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-9\n",
      "Epoch 9 Loss 2.4161 Accuracy 0.2349\n",
      "Time taken for 1 epoch: 232.11029291152954 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-10\n",
      "Epoch 10 Loss 2.3118 Accuracy 0.2473\n",
      "Time taken for 1 epoch: 243.56048583984375 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-11\n",
      "Epoch 11 Loss 2.2130 Accuracy 0.2591\n",
      "Time taken for 1 epoch: 229.8275809288025 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-12\n",
      "Epoch 12 Loss 2.1173 Accuracy 0.2715\n",
      "Time taken for 1 epoch: 224.46189522743225 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-13\n",
      "Epoch 13 Loss 2.0282 Accuracy 0.2833\n",
      "Time taken for 1 epoch: 1549.2386119365692 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-14\n",
      "Epoch 14 Loss 1.9431 Accuracy 0.2955\n",
      "Time taken for 1 epoch: 234.4206259250641 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-15\n",
      "Epoch 15 Loss 1.8660 Accuracy 0.3064\n",
      "Time taken for 1 epoch: 223.55828189849854 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-3f48c71a283c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# 每個 epoch 完成就存一次檔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定義我們要看幾遍數據集\n",
    "EPOCHS = 30\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# 用來寫資訊到 TensorBoard，非必要但十分推薦\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  # 重置紀錄 TensorBoard 的 metrics\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 \n",
    "  for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "    \n",
    "    # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "    train_step(inp, tar)  \n",
    "\n",
    "  # 每個 epoch 完成就存一次檔    \n",
    "  if (epoch + 1) % 1 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "    tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "  print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "  print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
