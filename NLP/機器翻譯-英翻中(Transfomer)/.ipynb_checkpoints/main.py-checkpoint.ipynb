{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "np.set_printoptions(suppress=True)\n",
    "logging.basicConfig(level=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NamedSplit('train'): ['newscommentary_v14',\n",
      "                       'wikititles_v1',\n",
      "                       'uncorpus_v1',\n",
      "                       'casia2015',\n",
      "                       'casict2011',\n",
      "                       'casict2015',\n",
      "                       'datum2015',\n",
      "                       'datum2017',\n",
      "                       'neu2017'],\n",
      " NamedSplit('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "  version=tfds.core.Version('0.0.3', experiments={tfds.core.Experiment.S3: False}),\n",
    "  language_pair=(\"zh\", \"en\"),\n",
    "  subsets={\n",
    "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "  }\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NamedSplit('train')(tfds.percent[0:20]),\n",
       " NamedSplit('train')(tfds.percent[20:21]),\n",
       " NamedSplit('train')(tfds.percent[21:100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perc = 20\n",
    "val_prec = 1\n",
    "drop_prec = 100 - train_perc - val_prec\n",
    "\n",
    "split = tfds.Split.TRAIN.subsplit([train_perc, val_prec, drop_prec])\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=split, as_supervised=True)\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Making Do With More', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'If the Putins, Erdo\\xc4\\x9fans, and Orb\\xc3\\xa1ns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe6\\x99\\xae\\xe4\\xba\\xac\\xe3\\x80\\x81\\xe5\\x9f\\x83\\xe5\\xb0\\x94\\xe5\\xa4\\x9a\\xe5\\xae\\x89\\xe5\\x92\\x8c\\xe6\\xac\\xa7\\xe5\\xb0\\x94\\xe7\\x8f\\xad\\xe5\\xb8\\x8c\\xe6\\x9c\\x9b\\xe7\\xbb\\xa7\\xe7\\xbb\\xad\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe5\\xbc\\x80\\xe6\\x94\\xbe\\xe5\\x9b\\xbd\\xe9\\x99\\x85\\xe4\\xbd\\x93\\xe7\\xb3\\xbb\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe7\\x9a\\x84\\xe7\\xbb\\x8f\\xe6\\xb5\\x8e\\xe5\\x88\\xa9\\xe7\\x9b\\x8a\\xef\\xbc\\x8c\\xe5\\xb0\\xb1\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe7\\xae\\x80\\xe5\\x8d\\x95\\xe5\\x9c\\xb0\\xe5\\x88\\xb6\\xe5\\xae\\x9a\\xe8\\x87\\xaa\\xe5\\xb7\\xb1\\xe7\\x9a\\x84\\xe8\\xa7\\x84\\xe5\\x88\\x99\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\x8f\\xaa\\xe6\\x9c\\x89\\xe5\\x9c\\xa8\\xe5\\x8f\\x91\\xe7\\x94\\x9f\\xe6\\xb7\\xb1\\xe5\\xba\\xa6\\xe8\\x90\\xa7\\xe6\\x9d\\xa1\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe5\\x8f\\x8d\\xe5\\xb8\\xb8\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe6\\x97\\xb6\\xef\\xbc\\x8c\\xe8\\xbf\\x99\\xe4\\xb8\\x80\\xe4\\xb8\\x8a\\xe9\\x99\\x90\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\x81\\x9a\\xe5\\x87\\xba\\xe8\\xb0\\x83\\xe6\\x95\\xb4\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe4\\xbe\\xbf\\xe8\\xae\\xa9\\xe5\\x8f\\x8d\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x94\\xbf\\xe7\\xad\\x96\\xe5\\xae\\x9e\\xe6\\x96\\xbd\\xe8\\xb6\\xb3\\xe5\\xa4\\x9f\\xe7\\x9a\\x84\\xe9\\x95\\xbf\\xe5\\xba\\xa6\\xef\\xbc\\x8c\\xe4\\xbd\\xbf\\xe4\\xba\\xba\\xe4\\xbb\\xac\\xe4\\xb8\\x80\\xe8\\x87\\xb4\\xe8\\xae\\xa4\\xe4\\xb8\\xba\\xe5\\xa2\\x9e\\xe5\\x8a\\xa0\\xe7\\x9a\\x84\\xe8\\xb5\\xa4\\xe5\\xad\\x97\\xe6\\x98\\xaf\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xef\\xbc\\x8c\\xe8\\x80\\x8c\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Do With More\n",
      "多劳应多得\n",
      "----------\n",
      "If the Putins, Erdoğans, and Orbáns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.\n",
      "如果普京、埃尔多安和欧尔班希望继续享有开放国际体系提供的经济利益，就不能简单地制定自己的规则。\n",
      "----------\n",
      "This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.\n",
      "只有在发生深度萧条或其他反常事件时，这一上限才能做出调整，以便让反周期政策实施足够的长度，使人们一致认为增加的赤字是周期性的，而不是结构性的。\n",
      "----------\n",
      "Fascist and communist regimes of the past, which followed a similar instrumentalist approach to democracy, come to mind here.\n",
      "在此我们想起了过去的法西斯主义和共产主义。 它们都相似地将民主作为实现其目的的工具。\n",
      "----------\n",
      "This phase culminated with the collapse of communism in 1989, but the chance to overcome the Continent’s historical divisions now required a redefinition of the European project.\n",
      "这种状态随着1989年共产主义崩溃而达至巅峰，但是克服欧洲大陆历史性分裂的机遇现在需要重新定义欧洲计划。\n",
      "----------\n",
      "The eurozone’s collapse (and, for all practical purposes, that of the EU itself) forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "----------\n",
      "With energy and enthusiasm, Burden turned that operation into a thriving health (not health-care) agency that covers three cities and about 300,000 people on the western edge of Los Angeles.\n",
      "在能量与激情的推动下，波顿将BCHD打造成了欣欣向荣的健康（而非医疗）机构，其服务范围覆盖了洛杉矶西侧三座城市的30万人。\n",
      "----------\n",
      "The result could be a world of fragmented blocs – an outcome that would undermine not only global prosperity, but also cooperation on shared challenges.\n",
      "其结果可能是一个四分五裂的世界 — — 这一结果不但会破坏全球繁荣，也会破坏面对共同挑战的合作。\n",
      "----------\n",
      "Among the questions being asked by NGOs, the UN, and national donors is how to prevent the recurrence of past mistakes.\n",
      "现在NGO们、联合国和捐助国们问得最多的一个问题就是如何避免再犯过去的错误。\n",
      "----------\n",
      "Managing the rise of NCDs will require long-term thinking, and government leaders will have to make investments that might pay off only after they are no longer in office.\n",
      "管理NCD的增加需要长期思维，政府领导人必须进行要在他们离任多年后才能收回成本的投资。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "  en = en_t.numpy().decode(\"utf-8\")\n",
    "  zh = zh_t.numpy().decode(\"utf-8\")\n",
    "  \n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)\n",
    "  \n",
    "  # 之後用來簡單評估模型的訓練情況\n",
    "  sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8135\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'that_', 'is_']\n",
      "\n",
      "CPU times: user 1min 30s, sys: 12.3 s, total: 1min 42s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "  print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart\n",
    "  subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2700, 7911, 10, 2942, 7457, 1163, 7925]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 2700     Taiwan\n",
      " 7911      \n",
      "   10     is \n",
      " 2942     bea\n",
      " 7457     uti\n",
      " 1163     ful\n",
      " 7925     .\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "  subword = subword_encoder_en.decode([idx])\n",
    "  print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4201\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "CPU times: user 7min 46s, sys: 11.6 s, total: 7min 58s\n",
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "  print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart \n",
    "  subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多劳应多得\n",
      "[48, 557, 116, 48, 81]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[17, 965, 11, 6, 1707, 676, 8, 211, 2712, 6683, 249, 3, 85, 1447, 7925]\n",
      "[45, 206, 171, 1, 847, 197, 236, 604, 45, 90, 17, 130, 102, 36, 7, 284, 80, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8135\n",
      "英文 EOS 的 index： 8136\n",
      "中文 BOS 的 index： 4201\n",
      "中文 EOS 的 index： 4202\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Making Do With More'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8135, 4682, 19, 717, 7911, 298, 2701, 7980, 8136],\n",
      " [4201, 48, 557, 116, 48, 81, 4202])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-23-c5074703b8a5>:5 encode  *\n        en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-81637afc26ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   2302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 2304\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   2305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m       return DatasetV1Adapter(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3888\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-23-c5074703b8a5>:5 encode  *\n        en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_examples.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([8135 4682   19  717 7911  298 2701 7980 8136], shape=(9,), dtype=int64)\n",
      "tf.Tensor([4201   48  557  116   48   81 4202], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29914 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "  cond1 = len(en_indices) <= MAX_LENGTH\n",
    "  cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "  assert cond1 and cond2\n",
    "  num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8135 4682   19 ...    0    0    0]\n",
      " [8135   17  965 ... 8136    0    0]\n",
      " [8135 6602    2 ...    0    0    0]\n",
      " ...\n",
      " [8135 1097  270 ...    0    0    0]\n",
      " [8135 1713   70 ...    0    0    0]\n",
      " [8135 2731 4553 ...    0    0    0]], shape=(64, 34), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4201   48  557 ...    0    0    0]\n",
      " [4201   45  206 ...    0    0    0]\n",
      " [4201   58    5 ...  683    3 4202]\n",
      " ...\n",
      " [4201   29  120 ...    0    0    0]\n",
      " [4201  297  161 ...    0    0    0]\n",
      " [4201  279  149 ... 4202    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8135  264  559 ...    0    0    0]\n",
      " [8135   39   41 ...    0    0    0]\n",
      " [8135   45  409 ...    0    0    0]\n",
      " ...\n",
      " [8135   45  346 ...    0    0    0]\n",
      " [8135   17 1410 ...    0    0    0]\n",
      " [8135   89   10 ...    0    0    0]], shape=(128, 40), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4201  127   20 ...    0    0    0]\n",
      " [4201   35    6 ...    0    0    0]\n",
      " [4201  120  340 ...    0    0    0]\n",
      " ...\n",
      " [4201   15   14 ...    0    0    0]\n",
      " [4201   45   90 ...    0    0    0]\n",
      " [4201  240   54 ...    3 4202    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.00872516,  0.03628476, -0.03393974,  0.00709321],\n",
       "         [-0.03939278, -0.0301337 , -0.00684045,  0.03930915],\n",
       "         [ 0.04217876,  0.03556073,  0.00940926,  0.02619899],\n",
       "         [ 0.00395658, -0.02045913,  0.04225378,  0.03054985],\n",
       "         [-0.0017071 , -0.01169461, -0.03898996,  0.02977444],\n",
       "         [-0.01053456, -0.02597178,  0.0221846 , -0.03926188],\n",
       "         [ 0.00246267, -0.0197565 ,  0.03870387,  0.04282421],\n",
       "         [ 0.00246267, -0.0197565 ,  0.03870387,  0.04282421]],\n",
       " \n",
       "        [[-0.00872516,  0.03628476, -0.03393974,  0.00709321],\n",
       "         [-0.02387941,  0.03241142, -0.02551947, -0.04527463],\n",
       "         [-0.01562656,  0.01015867, -0.00413325, -0.02324653],\n",
       "         [-0.03823967, -0.01697657, -0.03591858,  0.03328835],\n",
       "         [-0.03155795,  0.00274887,  0.04581268,  0.04479903],\n",
       "         [ 0.00292424, -0.04440271, -0.01196776,  0.04736396],\n",
       "         [-0.0017071 , -0.01169461, -0.03898996,  0.02977444],\n",
       "         [-0.01053456, -0.02597178,  0.0221846 , -0.03926188]]],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[-0.03401262, -0.03672125,  0.00148769, -0.02200452],\n",
       "         [-0.00802097, -0.04834964, -0.02985298, -0.02165037],\n",
       "         [-0.04805389,  0.03293635, -0.03475545, -0.01825899],\n",
       "         [-0.01045249,  0.03329812,  0.01279325, -0.03139227],\n",
       "         [-0.00984706, -0.02574105, -0.01065782,  0.01140115],\n",
       "         [ 0.04679844,  0.01800437,  0.00799765,  0.04974586],\n",
       "         [ 0.00132418, -0.03924266, -0.00341817, -0.03768504],\n",
       "         [-0.00556985, -0.04969132, -0.04252383, -0.02408794],\n",
       "         [-0.00556985, -0.04969132, -0.04252383, -0.02408794],\n",
       "         [-0.00556985, -0.04969132, -0.04252383, -0.02408794]],\n",
       " \n",
       "        [[-0.03401262, -0.03672125,  0.00148769, -0.02200452],\n",
       "         [ 0.00608138,  0.03791398, -0.00718598,  0.00104474],\n",
       "         [-0.01411204,  0.02962438,  0.01688961,  0.02466543],\n",
       "         [ 0.03541957,  0.00966803,  0.0428454 ,  0.03259467],\n",
       "         [ 0.02886103,  0.03402147, -0.00361333, -0.03875045],\n",
       "         [ 0.00446288, -0.02858969,  0.0252871 ,  0.01449865],\n",
       "         [-0.03010942, -0.03628191, -0.03144275, -0.04487304],\n",
       "         [ 0.00266431, -0.03812676,  0.02856315,  0.04966343],\n",
       "         [ 0.04679844,  0.01800437,  0.00799765,  0.04974586],\n",
       "         [ 0.00132418, -0.03924266, -0.00341817, -0.03768504]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      " [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      " [-0.00556985 -0.04969132 -0.04252383 -0.02408794]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # 將 `q`、 `k` 做點積再 scale\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "  # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # 以注意權重對 v 做加權平均（weighted average）\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.37518066 0.37483087 0.3749091  0.4999044 ]\n",
      "  [0.37498498 0.3751402  0.3751197  0.4999625 ]\n",
      "  [0.37501425 0.37473822 0.37502044 0.49995366]\n",
      "  [0.37481582 0.37508464 0.3751502  0.50004935]\n",
      "  [0.37512243 0.37493312 0.37505168 0.49990326]\n",
      "  [0.37488103 0.3752157  0.37497765 0.5001095 ]\n",
      "  [0.37483    0.37506163 0.37517226 0.500027  ]\n",
      "  [0.37483    0.37506163 0.37517226 0.500027  ]]\n",
      "\n",
      " [[0.62518704 0.24983297 0.6250395  0.3749666 ]\n",
      "  [0.6252012  0.24961075 0.62512374 0.37471482]\n",
      "  [0.6250589  0.24984503 0.6250475  0.3748681 ]\n",
      "  [0.62508297 0.250073   0.6250168  0.37506855]\n",
      "  [0.62479925 0.25036332 0.6248135  0.37528878]\n",
      "  [0.62492156 0.25029588 0.6249556  0.37520885]\n",
      "  [0.625098   0.2500403  0.62503004 0.3750695 ]\n",
      "  [0.6249098  0.24994117 0.62504107 0.37484056]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.12518652 0.12500928 0.12507352 0.1248996  0.12509455 0.12490661\n",
      "   0.12491497 0.12491497]\n",
      "  [0.12491778 0.12518606 0.12482233 0.12501846 0.12504874 0.12490162\n",
      "   0.1250525  0.1250525 ]\n",
      "  [0.1250259  0.12486619 0.12521544 0.12501644 0.12497193 0.12483996\n",
      "   0.12503207 0.12503207]\n",
      "  [0.12480042 0.12501068 0.12496474 0.12512203 0.12489336 0.12493914\n",
      "   0.12513483 0.12513483]\n",
      "  [0.12504514 0.1250909  0.12497015 0.12494324 0.12513404 0.12486787\n",
      "   0.12497437 0.12497437]\n",
      "  [0.12491287 0.12499937 0.12489377 0.1250447  0.12492347 0.12520683\n",
      "   0.12500949 0.12500949]\n",
      "  [0.12480078 0.12502968 0.12496535 0.12511979 0.12490946 0.12488896\n",
      "   0.12514298 0.12514298]\n",
      "  [0.12480078 0.12502968 0.12496535 0.12511979 0.12490946 0.12488896\n",
      "   0.12514298 0.12514298]]\n",
      "\n",
      " [[0.1251336  0.12509197 0.12500137 0.12504466 0.1249175  0.12491547\n",
      "   0.12504166 0.12485382]\n",
      "  [0.12510405 0.1252538  0.12509975 0.12496921 0.12483636 0.12477439\n",
      "   0.12494024 0.12502226]\n",
      "  [0.12502307 0.12510937 0.12504962 0.12498054 0.1249487  0.12489633\n",
      "   0.12495412 0.12503818]\n",
      "  [0.12499558 0.12490809 0.12490979 0.1251817  0.12498514 0.12508784\n",
      "   0.12508827 0.12484362]\n",
      "  [0.12492548 0.12483229 0.12493499 0.12504221 0.12529902 0.1250643\n",
      "   0.12495244 0.12494925]\n",
      "  [0.124913   0.12475985 0.12487216 0.12513448 0.12505384 0.12524202\n",
      "   0.12511837 0.12490624]\n",
      "  [0.12502918 0.12491568 0.12491994 0.12512489 0.12493196 0.12510835\n",
      "   0.12511808 0.12485193]\n",
      "  [0.12489939 0.1250558  0.1250621  0.12493823 0.12498687 0.12495431\n",
      "   0.12490998 0.12519331]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16687751 0.16664125 0.16672687 0.16649503 0.1667549  0.16650438\n",
      "   0.         0.        ]\n",
      "  [0.16658036 0.16693811 0.16645308 0.16671462 0.16675499 0.16655882\n",
      "   0.         0.        ]\n",
      "  [0.16671546 0.16650249 0.1669682  0.16670282 0.16664349 0.16646752\n",
      "   0.         0.        ]\n",
      "  [0.16646041 0.16674086 0.16667958 0.16688935 0.16658437 0.16664544\n",
      "   0.         0.        ]\n",
      "  [0.16671544 0.16677645 0.16661547 0.16657959 0.16683397 0.1664791\n",
      "   0.         0.        ]\n",
      "  [0.16655472 0.16667004 0.16652924 0.16673048 0.16656885 0.16694666\n",
      "   0.         0.        ]\n",
      "  [0.16646451 0.16676985 0.16668403 0.16689003 0.16660948 0.16658214\n",
      "   0.         0.        ]\n",
      "  [0.16646451 0.16676985 0.16668403 0.16689003 0.16660948 0.16658214\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.1251336  0.12509197 0.12500137 0.12504466 0.1249175  0.12491547\n",
      "   0.12504166 0.12485382]\n",
      "  [0.12510405 0.1252538  0.12509975 0.12496921 0.12483636 0.12477439\n",
      "   0.12494024 0.12502226]\n",
      "  [0.12502307 0.12510937 0.12504962 0.12498054 0.1249487  0.12489633\n",
      "   0.12495412 0.12503818]\n",
      "  [0.12499558 0.12490809 0.12490979 0.1251817  0.12498514 0.12508784\n",
      "   0.12508827 0.12484362]\n",
      "  [0.12492548 0.12483229 0.12493499 0.12504221 0.12529902 0.1250643\n",
      "   0.12495244 0.12494925]\n",
      "  [0.124913   0.12475985 0.12487216 0.12513448 0.12505384 0.12524202\n",
      "   0.12511837 0.12490624]\n",
      "  [0.12502918 0.12491568 0.12491994 0.12512489 0.12493196 0.12510835\n",
      "   0.12511808 0.12485193]\n",
      "  [0.12489939 0.1250558  0.1250621  0.12493823 0.12498687 0.12495431\n",
      "   0.12490998 0.12519331]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12504166, 0.12485382],\n",
       "        [0.12494024, 0.12502226],\n",
       "        [0.12495412, 0.12503818],\n",
       "        [0.12508827, 0.12484362],\n",
       "        [0.12495244, 0.12494925],\n",
       "        [0.12511837, 0.12490624],\n",
       "        [0.12511808, 0.12485193],\n",
       "        [0.12490998, 0.12519331]]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [-0.00802097 -0.04834964 -0.02985298 -0.02165037]\n",
      "  [-0.04805389  0.03293635 -0.03475545 -0.01825899]\n",
      "  [-0.01045249  0.03329812  0.01279325 -0.03139227]\n",
      "  [-0.00984706 -0.02574105 -0.01065782  0.01140115]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]]\n",
      "\n",
      " [[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [ 0.00608138  0.03791398 -0.00718598  0.00104474]\n",
      "  [-0.01411204  0.02962438  0.01688961  0.02466543]\n",
      "  [ 0.03541957  0.00966803  0.0428454   0.03259467]\n",
      "  [ 0.02886103  0.03402147 -0.00361333 -0.03875045]\n",
      "  [ 0.00446288 -0.02858969  0.0252871   0.01449865]\n",
      "  [-0.03010942 -0.03628191 -0.03144275 -0.04487304]\n",
      "  [ 0.00266431 -0.03812676  0.02856315  0.04966343]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.4998398  0.5001602  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33313262 0.33304116 0.3338263  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24989563 0.24976186 0.25013125 0.25021127 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.20006233 0.2001005  0.19993976 0.19983621 0.2000613  0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16646475 0.16652916 0.16650583 0.16663018 0.16670573 0.1671644\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14296137 0.14300315 0.14276354 0.14278981 0.14284588 0.14262104\n",
      "   0.14301524 0.         0.         0.        ]\n",
      "  [0.12505664 0.12516657 0.12493589 0.1248152  0.1249961  0.1247335\n",
      "   0.12508884 0.12520729 0.         0.        ]\n",
      "  [0.11114098 0.11123868 0.11103366 0.1109264  0.11108718 0.1108538\n",
      "   0.1111696  0.11127487 0.11127487 0.        ]\n",
      "  [0.10001215 0.10010006 0.09991557 0.09981905 0.09996373 0.09975372\n",
      "   0.1000379  0.10013262 0.10013262 0.10013262]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.499605   0.500395   0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33304656 0.33339098 0.3335625  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24960926 0.24992414 0.2500499  0.25041673 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19979988 0.20008326 0.19989654 0.19993149 0.20028886 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16670558 0.16655228 0.16664375 0.16677403 0.16652949 0.16679482\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14309436 0.14276016 0.14269535 0.14255705 0.14284074 0.1428199\n",
      "   0.14323239 0.         0.         0.        ]\n",
      "  [0.12498485 0.1248698  0.12500243 0.12512924 0.12476588 0.12512773\n",
      "   0.12485477 0.12526537 0.         0.        ]\n",
      "  [0.11088934 0.11112785 0.11114305 0.11128544 0.1110748  0.11110874\n",
      "   0.11082225 0.11119319 0.11135541 0.        ]\n",
      "  [0.10010771 0.09992195 0.09988828 0.09991132 0.10000543 0.1000214\n",
      "   0.10015589 0.09997317 0.0998694  0.10014543]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.4998398, 0.5001602, 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       ],\n",
       "       [0.499605 , 0.500395 , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.03939278 -0.0301337  -0.00684045  0.03930915]\n",
      "  [ 0.04217876  0.03556073  0.00940926  0.02619899]\n",
      "  [ 0.00395658 -0.02045913  0.04225378  0.03054985]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]]\n",
      "\n",
      " [[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.02387941  0.03241142 -0.02551947 -0.04527463]\n",
      "  [-0.01562656  0.01015867 -0.00413325 -0.02324653]\n",
      "  [-0.03823967 -0.01697657 -0.03591858  0.03328835]\n",
      "  [-0.03155795  0.00274887  0.04581268  0.04479903]\n",
      "  [ 0.00292424 -0.04440271 -0.01196776  0.04736396]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.00872516  0.03628476]\n",
      "   [-0.03939278 -0.0301337 ]\n",
      "   [ 0.04217876  0.03556073]\n",
      "   [ 0.00395658 -0.02045913]\n",
      "   [-0.0017071  -0.01169461]\n",
      "   [-0.01053456 -0.02597178]\n",
      "   [ 0.00246267 -0.0197565 ]\n",
      "   [ 0.00246267 -0.0197565 ]]\n",
      "\n",
      "  [[-0.03393974  0.00709321]\n",
      "   [-0.00684045  0.03930915]\n",
      "   [ 0.00940926  0.02619899]\n",
      "   [ 0.04225378  0.03054985]\n",
      "   [-0.03898996  0.02977444]\n",
      "   [ 0.0221846  -0.03926188]\n",
      "   [ 0.03870387  0.04282421]\n",
      "   [ 0.03870387  0.04282421]]]\n",
      "\n",
      "\n",
      " [[[-0.00872516  0.03628476]\n",
      "   [-0.02387941  0.03241142]\n",
      "   [-0.01562656  0.01015867]\n",
      "   [-0.03823967 -0.01697657]\n",
      "   [-0.03155795  0.00274887]\n",
      "   [ 0.00292424 -0.04440271]\n",
      "   [-0.0017071  -0.01169461]\n",
      "   [-0.01053456 -0.02597178]]\n",
      "\n",
      "  [[-0.03393974  0.00709321]\n",
      "   [-0.02551947 -0.04527463]\n",
      "   [-0.00413325 -0.02324653]\n",
      "   [-0.03591858  0.03328835]\n",
      "   [ 0.04581268  0.04479903]\n",
      "   [-0.01196776  0.04736396]\n",
      "   [-0.03898996  0.02977444]\n",
      "   [ 0.0221846  -0.03926188]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "  # x.shape: (batch_size, seq_len, d_model)\n",
    "  batch_size = tf.shape(x)[0]\n",
    "  \n",
    "  # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "  assert d_model % num_heads == 0\n",
    "  depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "  # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "  # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "  # (batch_size, seq_len, num_heads, depth)\n",
    "  reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "  # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "  # (batch_size, num_heads, seq_len, depth)\n",
    "  output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "  \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[-0.01000656  0.0136093   0.01146611 -0.0000503 ]\n",
      "  [-0.00998217  0.01357744  0.01145472 -0.00002868]\n",
      "  [-0.00996785  0.01356275  0.0114211  -0.00005009]\n",
      "  [-0.00995941  0.01354941  0.01142498 -0.00002872]\n",
      "  [-0.00998485  0.01357907  0.01144329 -0.00004433]\n",
      "  [-0.00999995  0.01359164  0.01145778 -0.00003175]\n",
      "  [-0.00995556  0.01354592  0.01142278 -0.00002877]\n",
      "  [-0.00995556  0.01354592  0.01142278 -0.00002877]]\n",
      "\n",
      " [[ 0.00610415 -0.01002899 -0.01386858  0.00167259]\n",
      "  [ 0.00606181 -0.00998417 -0.01382717  0.00168069]\n",
      "  [ 0.00609498 -0.01002481 -0.01385703  0.00168881]\n",
      "  [ 0.00613258 -0.01006723 -0.01388553  0.00169558]\n",
      "  [ 0.00615591 -0.01009651 -0.01390056  0.00171272]\n",
      "  [ 0.00617421 -0.01011746 -0.01393264  0.00169231]\n",
      "  [ 0.00614059 -0.01007446 -0.01390366  0.00167899]\n",
      "  [ 0.00610405 -0.01004137 -0.01386531  0.0017017 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.03939278 -0.0301337  -0.00684045  0.03930915]\n",
      "  [ 0.04217876  0.03556073  0.00940926  0.02619899]\n",
      "  [ 0.00395658 -0.02045913  0.04225378  0.03054985]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]\n",
      "  [ 0.00246267 -0.0197565   0.03870387  0.04282421]]\n",
      "\n",
      " [[-0.00872516  0.03628476 -0.03393974  0.00709321]\n",
      "  [-0.02387941  0.03241142 -0.02551947 -0.04527463]\n",
      "  [-0.01562656  0.01015867 -0.00413325 -0.02324653]\n",
      "  [-0.03823967 -0.01697657 -0.03591858  0.03328835]\n",
      "  [-0.03155795  0.00274887  0.04581268  0.04479903]\n",
      "  [ 0.00292424 -0.04440271 -0.01196776  0.04736396]\n",
      "  [-0.0017071  -0.01169461 -0.03898996  0.02977444]\n",
      "  [-0.01053456 -0.02597178  0.0221846  -0.03926188]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.81658655  1.4364216  -1.044008    0.424173  ]\n",
      "  [-1.1373439  -0.13116369 -0.33648854  1.6049961 ]\n",
      "  [-0.07380982  1.1580226  -1.5584166   0.47420377]\n",
      "  [-0.22442523 -1.4397199   0.33601767  1.3281274 ]\n",
      "  [-0.2970155   0.57074887 -1.468887    1.1951536 ]\n",
      "  [ 0.4581663  -0.404145    1.3193328  -1.3733542 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]]\n",
      "\n",
      " [[-0.8516326   1.4478076  -1.0054984   0.4093235 ]\n",
      "  [-0.5616184   1.5969627  -1.0646287   0.02928439]\n",
      "  [-0.96634096  1.6015658  -0.7045317   0.06930672]\n",
      "  [-1.12946     0.7941866  -0.84166455  1.176938  ]\n",
      "  [-1.645265    0.25666368  0.33159477  1.0570066 ]\n",
      "  [ 0.10427634 -0.94059813 -0.7554399   1.5917617 ]\n",
      "  [-0.4170378   0.67471915 -1.4206665   1.1629851 ]\n",
      "  [ 0.26112485 -0.22908711  1.3766065  -1.4086442 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    # 3 個 sub-layers 的主角們\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    # 定義每個 sub-layer 用的 LayerNorm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 定義每個 sub-layer 用的 Dropout\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "    # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "    # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "    # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "    # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "    # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "    # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "    # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [-0.00802097 -0.04834964 -0.02985298 -0.02165037]\n",
      "  [-0.04805389  0.03293635 -0.03475545 -0.01825899]\n",
      "  [-0.01045249  0.03329812  0.01279325 -0.03139227]\n",
      "  [-0.00984706 -0.02574105 -0.01065782  0.01140115]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]\n",
      "  [-0.00556985 -0.04969132 -0.04252383 -0.02408794]]\n",
      "\n",
      " [[-0.03401262 -0.03672125  0.00148769 -0.02200452]\n",
      "  [ 0.00608138  0.03791398 -0.00718598  0.00104474]\n",
      "  [-0.01411204  0.02962438  0.01688961  0.02466543]\n",
      "  [ 0.03541957  0.00966803  0.0428454   0.03259467]\n",
      "  [ 0.02886103  0.03402147 -0.00361333 -0.03875045]\n",
      "  [ 0.00446288 -0.02858969  0.0252871   0.01449865]\n",
      "  [-0.03010942 -0.03628191 -0.03144275 -0.04487304]\n",
      "  [ 0.00266431 -0.03812676  0.02856315  0.04966343]\n",
      "  [ 0.04679844  0.01800437  0.00799765  0.04974586]\n",
      "  [ 0.00132418 -0.03924266 -0.00341817 -0.03768504]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.81658655  1.4364216  -1.044008    0.424173  ]\n",
      "  [-1.1373439  -0.13116369 -0.33648854  1.6049961 ]\n",
      "  [-0.07380982  1.1580226  -1.5584166   0.47420377]\n",
      "  [-0.22442523 -1.4397199   0.33601767  1.3281274 ]\n",
      "  [-0.2970155   0.57074887 -1.468887    1.1951536 ]\n",
      "  [ 0.4581663  -0.404145    1.3193328  -1.3733542 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]\n",
      "  [-0.2991056  -1.2429054   0.00392848  1.5380825 ]]\n",
      "\n",
      " [[-0.8516326   1.4478076  -1.0054984   0.4093235 ]\n",
      "  [-0.5616184   1.5969627  -1.0646287   0.02928439]\n",
      "  [-0.96634096  1.6015658  -0.7045317   0.06930672]\n",
      "  [-1.12946     0.7941866  -0.84166455  1.176938  ]\n",
      "  [-1.645265    0.25666368  0.33159477  1.0570066 ]\n",
      "  [ 0.10427634 -0.94059813 -0.7554399   1.5917617 ]\n",
      "  [-0.4170378   0.67471915 -1.4206665   1.1629851 ]\n",
      "  [ 0.26112485 -0.22908711  1.3766065  -1.4086442 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[ 0.1504772  -1.6583127   0.54111654  0.966719  ]\n",
      "  [ 0.89563245 -1.594563   -0.10383192  0.80276245]\n",
      "  [-0.97793126  1.4678713   0.37510243 -0.86504257]\n",
      "  [-0.31977004  0.8672132   0.9501734  -1.4976165 ]\n",
      "  [ 0.4259625  -1.3991169  -0.34634256  1.3194968 ]\n",
      "  [ 0.8982514  -0.82325643 -1.1583406   1.0833455 ]\n",
      "  [ 1.2488142  -1.5478094   0.1445741   0.15442108]\n",
      "  [ 1.1827304  -1.2235019  -0.72236496  0.76313657]\n",
      "  [ 1.1827304  -1.2235022  -0.7223651   0.7631367 ]\n",
      "  [ 1.1827304  -1.2235022  -0.7223651   0.7631367 ]]\n",
      "\n",
      " [[ 0.1413997  -1.6384233   0.45268044  1.0443432 ]\n",
      "  [-0.15006247  1.616906   -1.1124184  -0.35442516]\n",
      "  [-1.6674784   0.9412033   0.17617717  0.550098  ]\n",
      "  [ 0.9220374  -1.5413343  -0.22996673  0.84926355]\n",
      "  [ 1.3146342   0.62156224 -0.89404845 -1.042148  ]\n",
      "  [ 0.41705227 -1.6228238   0.12041     1.0853614 ]\n",
      "  [ 1.3460523  -1.4763024   0.05538279  0.07486729]\n",
      "  [ 0.26538524 -1.4889268  -0.08254118  1.3060827 ]\n",
      "  [ 0.7809489  -0.82808274 -1.1390346   1.1861686 ]\n",
      "  [ 1.2466564  -1.5441262   0.0557915   0.24167821]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "    # 建立 `num_layers` 個 EncoderLayers\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "    # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "    # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "    input_seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "    # 再加上對應長度的位置編碼\n",
    "    x = self.embedding(x)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "    # 對 embedding 跟位置編碼的總合做 regularization\n",
    "    # 這在 Decoder 也會做\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    # 通過 N 個 EncoderLayer 做編碼\n",
    "    for i, enc_layer in enumerate(self.enc_layers):\n",
    "      x = enc_layer(x, training, mask)\n",
    "      # 以下只是用來 demo EncoderLayer outputs\n",
    "      #print('-' * 20)\n",
    "      #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "      \n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.80654085 -0.5846039  -0.3143984   1.7055432 ]\n",
      "  [-0.46891177 -0.5740812  -0.68403816  1.7270309 ]\n",
      "  [-0.3197091  -0.17782497 -1.1191479   1.6166819 ]\n",
      "  [-0.49274147  0.2699072  -1.2412686   1.464103  ]\n",
      "  [-0.88477206  0.16279443 -0.8493917   1.5713693 ]\n",
      "  [-0.9662535  -0.25279236 -0.45335218  1.6723982 ]\n",
      "  [-0.84764326 -0.5615214  -0.2887244   1.6978891 ]\n",
      "  [-0.61957765 -0.59192634 -0.51938546  1.7308894 ]]\n",
      "\n",
      " [[-0.80838835 -0.56457365 -0.33460823  1.7075703 ]\n",
      "  [-0.50152004 -0.5214133  -0.7037289   1.7266624 ]\n",
      "  [-0.34244877 -0.11313806 -1.144456    1.6000429 ]\n",
      "  [-0.50724393  0.21401617 -1.205033    1.4982607 ]\n",
      "  [-0.8861126   0.26368496 -0.9036027   1.5260304 ]\n",
      "  [-0.96629447 -0.21083693 -0.49055362  1.667685  ]\n",
      "  [-0.8683281  -0.53832126 -0.28836071  1.6950102 ]\n",
      "  [-0.6246324  -0.5758679  -0.5305908   1.7310913 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
