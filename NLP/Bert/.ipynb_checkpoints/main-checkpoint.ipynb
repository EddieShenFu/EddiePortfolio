{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.4.0\n"
     ]
    }
   ],
   "source": [
    "# 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  \n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token               index          \n",
      "-------------------------\n",
      "##錮                 20153\n",
      "##挟                 15968\n",
      "膽                    5615\n",
      "jean                10483\n",
      "##並                 13756\n",
      "##猎                 17390\n",
      "##ンス                12090\n",
      "皇                    4640\n",
      "##卦                 14365\n",
      "洹                    3832\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ㄅ', 647)\n",
      "('ㄆ', 648)\n",
      "('ㄇ', 649)\n",
      "('ㄉ', 650)\n",
      "('ㄋ', 651)\n",
      "('ㄌ', 652)\n",
      "('ㄍ', 653)\n",
      "('ㄎ', 654)\n",
      "('ㄏ', 655)\n",
      "('ㄒ', 656)\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(647, 657))\n",
    "some_pairs = [(t, idx) for t, idx in vocab.items() if idx in indices]\n",
    "for pair in some_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\n",
      "['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ...\n"
     ]
    }
   ],
   "source": [
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens[:10], '...')\n",
    "print(ids[:10], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入 tokens ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "--------------------------------------------------\n",
      "Top 1 (67%)：['[CLS]', '等', '到', '潮', '水', '來', '了', '，', '就', '知'] ...\n",
      "Top 2 (25%)：['[CLS]', '等', '到', '潮', '水', '濕', '了', '，', '就', '知'] ...\n",
      "Top 3 ( 2%)：['[CLS]', '等', '到', '潮', '水', '過', '了', '，', '就', '知'] ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "# 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
    "tokens_tensor = torch.tensor([ids])  # (1, seq_len)\n",
    "segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()\n",
    "\n",
    "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
    "maskedLM_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    # (1, seq_len, num_hidden_units)\n",
    "del maskedLM_model\n",
    "\n",
    "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "# 顯示 top k 可能的字。一般我們就是取 top 1 當作預測值\n",
    "print(\"輸入 tokens ：\", tokens[:10], '...')\n",
    "print('-' * 50)\n",
    "for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "    tokens[masked_index] = t\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:10]), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bertviz_repo'...\n",
      "remote: Enumerating objects: 48, done.\u001b[K\n",
      "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 1061 (delta 25), reused 36 (delta 16), pack-reused 1013\u001b[K\n",
      "Receiving objects: 100% (1061/1061), 100.13 MiB | 1.64 MiB/s, done.\n",
      "Resolving deltas: 100% (674/674), done.\n"
     ]
    }
   ],
   "source": [
    "# 安裝 BertViz\n",
    "import sys\n",
    "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
    "if not 'bertviz_repo' in sys.path:\n",
    "  sys.path += ['bertviz_repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertviz.pytorch_transformers_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-93f31c50bef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_transformers_attn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbertviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_view\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 在 jupyter notebook 裡頭顯示 visualzation 的 helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertviz.pytorch_transformers_attn'"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from bertviz.pytorch_transformers_attn import BertModel, BertTokenizer\n",
    "from bertviz.head_view import show\n",
    "\n",
    "# 在 jupyter notebook 裡頭顯示 visualzation 的 helper\n",
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "sentence_a = \"The cat sat on the mat\"\n",
    "sentence_b = \"The cat lay on the rug\"\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "call_html()\n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記得我們是使用中文 BERT\n",
    "model_version = 'bert-base-chinese'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "\n",
    "# 情境 1 的句子\n",
    "sentence_a = \"胖虎叫大雄去買漫畫，\"\n",
    "sentence_b = \"回來慢了就打他。\"\n",
    "\n",
    "# 得到 tokens 後丟入 BERT 取得 attention\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "call_html()\n",
    "\n",
    "# 交給 BertViz 視覺化\n",
    "head_view(attention, tokens)\n",
    "\n",
    "# 注意：執行這段程式碼以後只會顯示下圖左側的結果。\n",
    "# 為了方便你比較，我把情境 2 的結果也同時附上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
